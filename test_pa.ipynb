{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Give me a one-sentence pitch for a TV show.\"\n",
    "    # \"Give me a one-sentence pitch for a TV show.\"\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.0, top_k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 12:14:20 llm_engine.py:178] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Llama-2-7b-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Llama-2-7b-hf, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-12 12:14:22 model_runner.py:764] Starting to load model meta-llama/Llama-2-7b-hf...\n",
      "INFO 08-12 12:14:22 weight_utils.py:231] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3da7d09342e4b6e8fd81ddb97f90c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 12:14:26 model_runner.py:776] Loading model weights took 12.5523 GB\n",
      "INFO 08-12 12:14:26 gpu_executor.py:104] # GPU blocks: 7445, # CPU blocks: 512\n",
      "INFO 08-12 12:14:32 model_runner.py:1095] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-12 12:14:32 model_runner.py:1099] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-12 12:14:43 model_runner.py:1296] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\", enable_prompt_adapter=True, max_prompt_adapter_token=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee5784ca8e84c31b55147e89ef5b965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from vllm.prompt_adapter.request import PromptAdapterRequest\n",
    "\n",
    "pa_path = snapshot_download(repo_id=\"swapnilbp/llama_tweet_ptune\")\n",
    "\n",
    "pa_req = PromptAdapterRequest(\"hate_speech\", 1, pa_path, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 43.14it/s, est. speed input: 521.15 toks/s, output: 43.37 toks/s]\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(\n",
    "    \"apples?\", \n",
    "    prompt_adapter_request=pa_req\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:07<00:00, 67.16s/it, est. speed input: 0.06 toks/s, output: 0.01 toks/s]\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(\n",
    "    \"apples?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
