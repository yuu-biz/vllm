{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VLLM_ATTENTION_BACKEND'] = 'FLASHINFER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 08:56:12 llm_engine.py:176] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B, use_v2_block_manager=True, enable_prefix_caching=False)\n",
      "INFO 08-20 08:56:14 model_runner.py:721] Starting to load model meta-llama/Meta-Llama-3-8B...\n",
      "INFO 08-20 08:56:14 weight_utils.py:231] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be49470171d44ff2850462113042db5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 08:56:21 model_runner.py:733] Loading model weights took 14.9595 GB\n",
      "INFO 08-20 08:56:22 gpu_executor.py:102] # GPU blocks: 11943, # CPU blocks: 2048\n",
      "INFO 08-20 08:56:25 model_runner.py:1025] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-20 08:56:25 model_runner.py:1029] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-20 08:56:36 model_runner.py:1226] Graph capturing finished in 11 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm.entrypoints.llm import LLM\n",
    "from vllm import SamplingParams\n",
    "\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B\",use_v2_block_manager=True)\n",
    "samplingparams = SamplingParams(temperature=0.0, max_tokens=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"\"\"\n",
    "The Future of Energy-Efficient Machine Learning: Challenges and Innovations\n",
    "\n",
    "In recent years, machine learning (ML) has evolved from a niche research area to a transformative technology, driving advancements across various industries, including healthcare, finance, and transportation. However, the rapid growth of ML has also brought to light a significant challenge: the environmental impact of energy consumption associated with running large-scale models. As ML systems become more complex and data-hungry, their energy demands soar, contributing to an increased carbon footprint. Addressing this issue is crucial for ensuring that the benefits of ML do not come at an unsustainable environmental cost.\n",
    "\n",
    "One of the primary factors driving the energy consumption of ML systems is the training process. Training large models, such as those used in natural language processing (NLP) and computer vision, requires vast amounts of computational power. This process involves iterating over massive datasets multiple times, which can take weeks or even months to complete. As a result, the energy required to power the servers and cooling systems in data centers can be substantial. Additionally, the need to maintain these models once they are deployed adds to the overall energy consumption.\n",
    "\n",
    "To mitigate the environmental impact of ML, researchers and engineers are exploring various approaches to make these systems more energy-efficient. One promising avenue is the development of new algorithms and architectures that require less computational power to achieve the same or better performance. For instance, model compression techniques, such as pruning and quantization, reduce the size of ML models by eliminating unnecessary parameters and lowering the precision of calculations. These techniques can significantly decrease the amount of energy needed for both training and inference without sacrificing accuracy.\n",
    "\n",
    "Another approach involves optimizing the hardware used for ML tasks. Traditional processors, such as CPUs and GPUs, are not always the most efficient for running ML workloads. Specialized hardware, like tensor processing units (TPUs) and application-specific integrated circuits (ASICs), are designed specifically for ML tasks and can perform computations more efficiently. By leveraging these specialized chips, it is possible to reduce the energy consumption of ML systems while also speeding up the training process.\n",
    "\n",
    "Moreover, researchers are also looking into the potential of distributed and edge computing to further enhance energy efficiency. Distributed computing involves spreading the computational load across multiple machines, which can reduce the strain on individual servers and improve overall efficiency. Edge computing, on the other hand, involves processing data closer to where it is generated, such as on local devices or edge servers. This approach minimizes the need to transfer large amounts of data to centralized data centers, thereby reducing energy consumption associated with data transmission and storage.\n",
    "\n",
    "In conclusion, while the energy demands of machine learning are a significant concern, ongoing research and innovation are paving the way for more sustainable solutions. By developing energy-efficient algorithms, optimizing hardware, and leveraging distributed and edge computing, the ML community can continue to advance the field without compromising the environment. As the world increasingly relies on ML to solve complex problems, it is imperative to prioritize energy efficiency to ensure that these technologies can be sustained in the long term.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What roles do specialized hardware, distributed computing, and edge computing play in enhancing the energy efficiency of machine learning systems?\",\n",
    "    \"How do model compression techniques, such as pruning and quantization, contribute to reducing the energy demands of ML systems?\",\n",
    "    \"What are some of the primary factors contributing to the high energy consumption of machine learning systems, particularly during the training process?\",\n",
    "    \"apples are delicious?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [base_prompt + x for x in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    }
   ],
   "source": [
    "output = llm.generate(\n",
    "    prompts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0].outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
