{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-15 12:52:49 llm_engine.py:178] Initializing an LLM engine (v0.5.4) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=mistralai/Mistral-7B-v0.1, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-15 12:52:50 selector.py:237] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-15 12:52:50 selector.py:117] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-15 12:52:52 model_runner.py:764] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "INFO 08-15 12:52:52 selector.py:237] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-15 12:52:52 selector.py:117] Using XFormers backend.\n",
      "INFO 08-15 12:52:52 weight_utils.py:231] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b7b3174b704b2ab60a64c422497c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-15 12:52:55 model_runner.py:776] Loading model weights took 13.4966 GB\n",
      "INFO 08-15 12:52:57 gpu_executor.py:104] # GPU blocks: 27444, # CPU blocks: 2048\n",
      "INFO 08-15 12:53:03 model_runner.py:1094] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-15 12:53:03 model_runner.py:1098] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-15 12:53:16 model_runner.py:1295] Graph capturing finished in 13 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm.entrypoints.llm import LLM\n",
    "\n",
    "\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-v0.1\", enable_control_vector=True, max_control_vectors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length manager 0\n",
      "Length 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it, est. speed input: 6.08 toks/s, output: 68.53 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='how do I destress? teach me please!', prompt_token_ids=[1, 910, 511, 315, 2620, 638, 28804, 3453, 528, 4665, 28808], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nI’m not sure if you have ever heard of the term “destressing” before, but it is a very important part of life. It can be hard to find time for yourself when there are so many things going on in your day-to-day routine and stress levels seem high all around us (especially during this pandemic). But don’t worry – we will help guide you through some easy ways that may work best with what works well at home or school/workplace settings where people tend towards being more relaxed than others might expect them too! Here are 10 tips:', token_ids=(13, 13, 28737, 28809, 28719, 459, 1864, 513, 368, 506, 2270, 3364, 302, 272, 1850, 981, 5356, 638, 288, 28838, 1159, 28725, 562, 378, 349, 264, 1215, 2278, 744, 302, 1411, 28723, 661, 541, 347, 1856, 298, 1300, 727, 354, 3936, 739, 736, 460, 579, 1287, 1722, 1404, 356, 297, 574, 1370, 28733, 532, 28733, 1466, 11935, 304, 6727, 6157, 1709, 1486, 544, 1401, 592, 325, 26160, 1938, 456, 15749, 609, 1092, 949, 28809, 28707, 7980, 764, 478, 622, 1316, 8327, 368, 1059, 741, 3411, 4342, 369, 993, 771, 1489, 395, 767, 3791, 1162, 438, 1611, 442, 2052, 28748, 1328, 2912, 6472, 970, 905, 6273, 5083, 1250, 680, 18788, 821, 2663, 1659, 1675, 706, 1368, 28808, 4003, 460, 28705, 28740, 28734, 10636, 28747, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1723726396.8013873, last_token_time=1723726396.8013873, first_scheduled_time=1723726396.8093219, first_token_time=1723726396.9044013, time_in_queue=0.0079345703125, finished_time=1723726398.6183047), lora_request=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.control_vectors.request import ControlVectorRequest\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "\n",
    "llm.generate(\n",
    "    \"how do I destress? teach me please!\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=1000),\n",
    "    control_vector_request=ControlVectorRequest(\n",
    "        \"ooooo\", 8, \"./trip.safetensors\", 0.2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate(\n",
    "    \"if life is hard, what should you do?\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
