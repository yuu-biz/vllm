{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 13:09:19 llm_engine.py:178] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3-8B', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-12 13:09:20 model_runner.py:764] Starting to load model meta-llama/Meta-Llama-3-8B...\n",
      "INFO 08-12 13:09:21 weight_utils.py:231] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9196daa868804c828e619a7617d5b2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 13:09:25 model_runner.py:776] Loading model weights took 14.9595 GB\n",
      "INFO 08-12 13:09:26 gpu_executor.py:104] # GPU blocks: 27955, # CPU blocks: 2048\n",
      "INFO 08-12 13:09:31 model_runner.py:1094] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-12 13:09:31 model_runner.py:1098] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-12 13:09:43 model_runner.py:1295] Graph capturing finished in 12 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm.entrypoints.llm import LLM\n",
    "\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B\", enable_control_vector=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it, est. speed input: 8.70 toks/s, output: 79.07 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=0, prompt='if life is hard, what should you do?', prompt_token_ids=[128000, 333, 2324, 374, 2653, 11, 1148, 1288, 499, 656, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" (part 1)\\nI have been thinking about this question a lot lately. I am not sure if it's because of the current state of our world or just my own personal circumstances but either way, there are many people who feel like they can't go on.\\nThe truth is that we all face challenges in life and sometimes those things seem insurmountable to us at first glance; however when faced with adversity head-on instead of running away from them then everything becomes easier than before!\\nLife\", token_ids=(320, 4581, 220, 16, 340, 40, 617, 1027, 7422, 922, 420, 3488, 264, 2763, 31445, 13, 358, 1097, 539, 2771, 422, 433, 596, 1606, 315, 279, 1510, 1614, 315, 1057, 1917, 477, 1120, 856, 1866, 4443, 13463, 719, 3060, 1648, 11, 1070, 527, 1690, 1274, 889, 2733, 1093, 814, 649, 956, 733, 389, 627, 791, 8206, 374, 430, 584, 682, 3663, 11774, 304, 2324, 323, 7170, 1884, 2574, 2873, 1672, 324, 16966, 481, 311, 603, 520, 1176, 34522, 26, 4869, 994, 17011, 449, 90930, 2010, 10539, 4619, 315, 4401, 3201, 505, 1124, 1243, 4395, 9221, 8831, 1109, 1603, 4999, 26833), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1723468184.574457, last_token_time=1723468184.574457, first_scheduled_time=1723468184.5798333, first_token_time=1723468184.631598, time_in_queue=0.005376338958740234, finished_time=1723468185.8442824), lora_request=None)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.control_vectors.request import ControlVectorRequest\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "\n",
    "llm.generate(\n",
    "    \"if life is hard, what should you do?\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=100),\n",
    "    # control_vector_request=ControlVectorRequest(\n",
    "    #     \"aooo\", 1, \"./trip.safetensors\", 0.3\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 8.81 toks/s, output: 80.09 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=3, prompt='if life is hard, what should you do?', prompt_token_ids=[128000, 333, 2324, 374, 2653, 11, 1148, 1288, 499, 656, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" (part 1)\\nI have been thinking about this question a lot lately. I am not sure if it's because of the current state of our world or just my own personal circumstances but either way, there are many people who feel like they can't go on.\\nThe truth is that we all face challenges in life and sometimes those things seem insurmountable to us at first glance; however when faced with adversity head-on instead of running away from them then everything becomes easier than before!\\nLife\", token_ids=(320, 4581, 220, 16, 340, 40, 617, 1027, 7422, 922, 420, 3488, 264, 2763, 31445, 13, 358, 1097, 539, 2771, 422, 433, 596, 1606, 315, 279, 1510, 1614, 315, 1057, 1917, 477, 1120, 856, 1866, 4443, 13463, 719, 3060, 1648, 11, 1070, 527, 1690, 1274, 889, 2733, 1093, 814, 649, 956, 733, 389, 627, 791, 8206, 374, 430, 584, 682, 3663, 11774, 304, 2324, 323, 7170, 1884, 2574, 2873, 1672, 324, 16966, 481, 311, 603, 520, 1176, 34522, 26, 4869, 994, 17011, 449, 90930, 2010, 10539, 4619, 315, 4401, 3201, 505, 1124, 1243, 4395, 9221, 8831, 1109, 1603, 4999, 26833), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1723468236.3464708, last_token_time=1723468236.3464708, first_scheduled_time=1723468236.3505945, first_token_time=1723468236.3791747, time_in_queue=0.004123687744140625, finished_time=1723468237.598669), lora_request=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(\n",
    "    \"if life is hard, what should you do?\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
