{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 12:09:56 llm_engine.py:178] Initializing an LLM engine (v0.5.4) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=mistralai/Mistral-7B-v0.1, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-20 12:09:57 selector.py:237] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-20 12:09:57 selector.py:117] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 12:09:59 model_runner.py:787] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "INFO 08-20 12:09:59 selector.py:237] Cannot use FlashAttention-2 backend due to sliding window.\n",
      "INFO 08-20 12:09:59 selector.py:117] Using XFormers backend.\n",
      "INFO 08-20 12:09:59 weight_utils.py:231] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05647c063f2c4be7b630475ac72b178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-20 12:10:03 model_runner.py:799] Loading model weights took 13.4966 GB\n",
      "INFO 08-20 12:10:08 gpu_executor.py:104] # GPU blocks: 11432, # CPU blocks: 2048\n",
      "INFO 08-20 12:10:11 model_runner.py:1117] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-20 12:10:11 model_runner.py:1121] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-20 12:10:25 model_runner.py:1318] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm.entrypoints.llm import LLM\n",
    "\n",
    "\n",
    "llm = LLM(model=\"mistralai/Mistral-7B-v0.1\", enable_control_vector=True, max_control_vectors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.09s/it, est. speed input: 1.22 toks/s, output: 35.74 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=5, prompt='what is life?', prompt_token_ids=[1, 767, 349, 1411, 28804], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nWhat a great day! I had the opportunity to go on an adventure with my friend, Katie. We went out for breakfast and then headed over to The Candy Apple Cafe & Magical Bakery in Disney Springs at Walt Disney World Resort. It was so much fun seeing all of their delicious treats that are inspired by some of our favorite characters from movies like Beauty and the Beast, Frozen 2, Toy Story 4 (which we saw last night), Aladdin…and more!! They also have specialty drinks such as “The Little Mermaid” which has blue raspberry lemonade mixed into it along with other flavors depending upon your preference – yum!!!', token_ids=(13, 3195, 264, 1598, 1370, 28808, 315, 553, 272, 5701, 298, 576, 356, 396, 15982, 395, 586, 1832, 28725, 524, 11541, 28723, 816, 2068, 575, 354, 11814, 304, 868, 11256, 754, 298, 415, 334, 10087, 10244, 11013, 1512, 567, 4642, 745, 23515, 1193, 297, 14665, 24846, 438, 27558, 14665, 3304, 1992, 419, 28723, 661, 403, 579, 1188, 746, 6252, 544, 302, 652, 15992, 27378, 369, 460, 11971, 486, 741, 302, 813, 6656, 6128, 477, 10615, 737, 16686, 28724, 304, 272, 1739, 529, 28725, 18760, 3039, 28705, 28750, 28725, 20944, 13689, 28705, 28781, 325, 6974, 478, 2672, 1432, 2125, 557, 976, 988, 262, 28878, 391, 680, 3946, 1306, 835, 506, 2841, 884, 16195, 1259, 390, 981, 1014, 9999, 5625, 705, 313, 28838, 690, 659, 5045, 408, 10124, 14233, 23598, 770, 9430, 778, 378, 2267, 395, 799, 15637, 734, 10085, 3714, 574, 21448, 764, 337, 383, 13915, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1724156215.5329041, last_token_time=1724156215.5329041, first_scheduled_time=1724156215.5365622, first_token_time=1724156215.5909336, time_in_queue=0.0036580562591552734, finished_time=1724156219.6213899), lora_request=None)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vllm.control_vectors.request import ControlVectorRequest\n",
    "from vllm.sampling_params import SamplingParams\n",
    "\n",
    "\n",
    "llm.generate(\n",
    "    \"what is life?\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=1000),\n",
    "    control_vector_request=ControlVectorRequest(\n",
    "        \"ooooo\", 21, \"./cool.gguf\", 0.8\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[RequestOutput(request_id=3, prompt='what is life?', prompt_token_ids=[1, 767, 349, 1411, 28804], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\nwhat is death?\\nis there a difference between the two, or are they one and the same thing?\\nif you were to die today would it be any different than if you died tomorrow. 10 years from now…50 years from now….would your life have been lived differently had that day come sooner rather then later?? i think not! we all live our lives in such a way as though each of us has an expiration date stamped on his forehead; but do we really know when this will happen for sure?! no matter how much money someone may make, he can’t buy more time with himself – only God knows exactly when He wants him gone (and even then sometimes people don’t get out alive). so why worry about something over which YOU HAVE NO CONTROL AT ALL!!', token_ids=(13, 6802, 349, 3168, 28804, 13, 278, 736, 264, 5133, 1444, 272, 989, 28725, 442, 460, 590, 624, 304, 272, 1348, 1970, 28804, 13, 335, 368, 654, 298, 1202, 3154, 682, 378, 347, 707, 1581, 821, 513, 368, 4847, 10759, 28723, 28705, 28740, 28734, 1267, 477, 1055, 28878, 28782, 28734, 1267, 477, 1055, 20541, 28727, 474, 574, 1411, 506, 750, 6262, 16841, 553, 369, 1370, 1567, 19960, 3210, 868, 2062, 8908, 613, 1073, 459, 28808, 478, 544, 2943, 813, 4621, 297, 1259, 264, 1069, 390, 2070, 1430, 302, 592, 659, 396, 2365, 8679, 3608, 341, 16734, 356, 516, 18522, 28745, 562, 511, 478, 1528, 873, 739, 456, 622, 4804, 354, 1864, 18023, 708, 3209, 910, 1188, 2445, 2493, 993, 1038, 28725, 400, 541, 28809, 28707, 3848, 680, 727, 395, 2722, 764, 865, 2499, 5960, 4668, 739, 650, 5659, 713, 4214, 325, 391, 1019, 868, 4662, 905, 949, 28809, 28707, 625, 575, 8630, 609, 579, 2079, 7980, 684, 1545, 754, 690, 15479, 22769, 7929, 4192, 28738, 19172, 9274, 17565, 3946, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1724156175.6550412, last_token_time=1724156175.6550412, first_scheduled_time=1724156175.6587863, first_token_time=1724156175.692492, time_in_queue=0.0037450790405273438, finished_time=1724156180.3944185), lora_request=None)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.generate(\n",
    "    \"if life is hard, what should you do?\",\n",
    "    sampling_params=SamplingParams(temperature=0.0, repetition_penalty=1.2, frequency_penalty=1.2, max_tokens=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
